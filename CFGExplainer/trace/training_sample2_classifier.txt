2023-05-31 21:52:26.370274: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-05-31 21:52:27.196414: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-05-31 21:52:27.196971: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2023-05-31 21:52:27.239059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-05-31 21:52:27.239174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3080 Ti computeCapability: 8.6
coreClock: 1.83GHz coreCount: 80 deviceMemorySize: 11.75GiB deviceMemoryBandwidth: 849.46GiB/s
2023-05-31 21:52:27.239189: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-05-31 21:52:27.246217: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-05-31 21:52:27.246273: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-05-31 21:52:27.247124: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2023-05-31 21:52:27.247291: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2023-05-31 21:52:27.249727: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2023-05-31 21:52:27.250076: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2023-05-31 21:52:27.250164: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2023-05-31 21:52:27.250216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-05-31 21:52:27.250316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-05-31 21:52:27.250378: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2023-05-31 21:58:38.607045: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-05-31 21:58:38.607235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-05-31 21:58:38.607353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3080 Ti computeCapability: 8.6
coreClock: 1.83GHz coreCount: 80 deviceMemorySize: 11.75GiB deviceMemoryBandwidth: 849.46GiB/s
2023-05-31 21:58:38.607372: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-05-31 21:58:38.607393: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-05-31 21:58:38.607398: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-05-31 21:58:38.607405: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2023-05-31 21:58:38.607410: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2023-05-31 21:58:38.607415: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2023-05-31 21:58:38.607426: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2023-05-31 21:58:38.607431: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2023-05-31 21:58:38.607457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-05-31 21:58:38.607548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-05-31 21:58:38.607603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2023-05-31 21:58:38.607617: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-05-31 21:58:38.905249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2023-05-31 21:58:38.905290: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2023-05-31 21:58:38.905296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2023-05-31 21:58:38.905481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-05-31 21:58:38.905626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-05-31 21:58:38.905698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-05-31 21:58:38.905771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 70 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6)
2023-05-31 21:58:38.906720: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 8388608000 exceeds 10% of free system memory.
2023-05-31 22:00:30.999314: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 2097152000 exceeds 10% of free system memory.
2023-05-31 22:00:37.202569: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 8388608000 exceeds 10% of free system memory.
2023-05-31 22:00:42.178383: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-05-31 22:00:42.560067: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-05-31 22:00:42.561423: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-05-31 22:00:42.561728: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 28.80M (30199040 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-05-31 22:00:42.562014: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 25.92M (27179264 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-05-31 22:00:42.562307: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 23.33M (24461568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-05-31 22:00:42.562590: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 21.00M (22015488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-05-31 22:00:42.562595: W tensorflow/core/common_runtime/bfc_allocator.cc:314] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.
2023-05-31 22:00:42.569925: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-05-31 22:00:42.570225: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-05-31 22:00:52.572073: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-05-31 22:00:52.573603: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-05-31 22:00:52.573640: W tensorflow/core/common_runtime/bfc_allocator.cc:433] Allocator (GPU_0_bfc) ran out of memory trying to allocate 20.00MiB (rounded to 20971520)requested by op BatchMatMulV2
Current allocation summary follows.
2023-05-31 22:00:52.573658: I tensorflow/core/common_runtime/bfc_allocator.cc:972] BFCAllocator dump for GPU_0_bfc
2023-05-31 22:00:52.573675: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (256): 	Total Chunks: 10, Chunks in use: 10. 2.5KiB allocated for chunks. 2.5KiB in use in bin. 60B client-requested in use in bin.
2023-05-31 22:00:52.573687: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (512): 	Total Chunks: 1, Chunks in use: 1. 512B allocated for chunks. 512B in use in bin. 512B client-requested in use in bin.
2023-05-31 22:00:52.573699: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (1024): 	Total Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.
2023-05-31 22:00:52.573711: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (2048): 	Total Chunks: 1, Chunks in use: 1. 2.0KiB allocated for chunks. 2.0KiB in use in bin. 2.0KiB client-requested in use in bin.
2023-05-31 22:00:52.573723: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (4096): 	Total Chunks: 1, Chunks in use: 1. 4.0KiB allocated for chunks. 4.0KiB in use in bin. 4.0KiB client-requested in use in bin.
2023-05-31 22:00:52.573734: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (8192): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-05-31 22:00:52.573746: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (16384): 	Total Chunks: 1, Chunks in use: 0. 23.8KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-05-31 22:00:52.573759: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (32768): 	Total Chunks: 2, Chunks in use: 1. 64.0KiB allocated for chunks. 32.0KiB in use in bin. 32.0KiB client-requested in use in bin.
2023-05-31 22:00:52.573770: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (65536): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-05-31 22:00:52.573783: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (131072): 	Total Chunks: 3, Chunks in use: 2. 512.0KiB allocated for chunks. 320.0KiB in use in bin. 320.0KiB client-requested in use in bin.
2023-05-31 22:00:52.573796: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (262144): 	Total Chunks: 1, Chunks in use: 1. 414.0KiB allocated for chunks. 414.0KiB in use in bin. 256.0KiB client-requested in use in bin.
2023-05-31 22:00:52.573807: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (524288): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-05-31 22:00:52.573818: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (1048576): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-05-31 22:00:52.573851: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (2097152): 	Total Chunks: 2, Chunks in use: 1. 4.00MiB allocated for chunks. 2.00MiB in use in bin. 2.00MiB client-requested in use in bin.
2023-05-31 22:00:52.573862: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (4194304): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-05-31 22:00:52.573873: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (8388608): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-05-31 22:00:52.573886: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (16777216): 	Total Chunks: 1, Chunks in use: 1. 16.00MiB allocated for chunks. 16.00MiB in use in bin. 10.00MiB client-requested in use in bin.
2023-05-31 22:00:52.573896: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (33554432): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-05-31 22:00:52.573907: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (67108864): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-05-31 22:00:52.573918: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (134217728): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-05-31 22:00:52.573929: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (268435456): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-05-31 22:00:52.573940: I tensorflow/core/common_runtime/bfc_allocator.cc:995] Bin for 20.00MiB was 16.00MiB, Chunk State: 
2023-05-31 22:00:52.573951: I tensorflow/core/common_runtime/bfc_allocator.cc:1008] Next region of size 1048576
2023-05-31 22:00:52.573966: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7f3441400000 of size 1280 next 1
2023-05-31 22:00:52.573977: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7f3441400500 of size 256 next 2
2023-05-31 22:00:52.573986: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7f3441400600 of size 256 next 8
2023-05-31 22:00:52.573995: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7f3441400700 of size 4096 next 9
2023-05-31 22:00:52.574005: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7f3441401700 of size 256 next 11
2023-05-31 22:00:52.574015: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7f3441401800 of size 2048 next 14
2023-05-31 22:00:52.574024: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7f3441402000 of size 256 next 16
2023-05-31 22:00:52.574034: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7f3441402100 of size 512 next 15
2023-05-31 22:00:52.574043: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7f3441402300 of size 256 next 18
2023-05-31 22:00:52.574052: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7f3441402400 of size 256 next 19
2023-05-31 22:00:52.574061: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7f3441402500 of size 256 next 20
2023-05-31 22:00:52.574071: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7f3441402600 of size 256 next 21
2023-05-31 22:00:52.574083: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] Free  at 7f3441402700 of size 24320 next 3
2023-05-31 22:00:52.574092: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7f3441408600 of size 256 next 4
2023-05-31 22:00:52.574101: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] Free  at 7f3441408700 of size 32768 next 5
2023-05-31 22:00:52.574110: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7f3441410700 of size 256 next 6
2023-05-31 22:00:52.574127: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7f3441410800 of size 32768 next 7
2023-05-31 22:00:52.574138: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7f3441418800 of size 163840 next 23
2023-05-31 22:00:52.574148: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7f3441440800 of size 163840 next 24
2023-05-31 22:00:52.574157: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] Free  at 7f3441468800 of size 196608 next 17
2023-05-31 22:00:52.574168: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7f3441498800 of size 423936 next 18446744073709551615
2023-05-31 22:00:52.574176: I tensorflow/core/common_runtime/bfc_allocator.cc:1008] Next region of size 4194304
2023-05-31 22:00:52.574186: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] Free  at 7f3441800000 of size 2097152 next 13
2023-05-31 22:00:52.574195: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7f3441a00000 of size 2097152 next 18446744073709551615
2023-05-31 22:00:52.574204: I tensorflow/core/common_runtime/bfc_allocator.cc:1008] Next region of size 16777216
2023-05-31 22:00:52.574213: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7f44ee000000 of size 16777216 next 18446744073709551615
2023-05-31 22:00:52.574222: I tensorflow/core/common_runtime/bfc_allocator.cc:1033]      Summary of in-use Chunks by size: 
2023-05-31 22:00:52.574234: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 10 Chunks of size 256 totalling 2.5KiB
2023-05-31 22:00:52.574244: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 512 totalling 512B
2023-05-31 22:00:52.574253: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 1280 totalling 1.2KiB
2023-05-31 22:00:52.574262: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 2048 totalling 2.0KiB
2023-05-31 22:00:52.574271: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 4096 totalling 4.0KiB
2023-05-31 22:00:52.574280: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 32768 totalling 32.0KiB
2023-05-31 22:00:52.574289: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 2 Chunks of size 163840 totalling 320.0KiB
2023-05-31 22:00:52.574300: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 423936 totalling 414.0KiB
2023-05-31 22:00:52.574309: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 2097152 totalling 2.00MiB
2023-05-31 22:00:52.574319: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 16777216 totalling 16.00MiB
2023-05-31 22:00:52.574328: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Sum Total of in-use chunks: 18.76MiB
2023-05-31 22:00:52.574337: I tensorflow/core/common_runtime/bfc_allocator.cc:1042] total_region_allocated_bytes_: 22020096 memory_limit_: 74186752 available bytes: 52166656 curr_region_allocation_bytes_: 33554432
2023-05-31 22:00:52.574352: I tensorflow/core/common_runtime/bfc_allocator.cc:1048] Stats: 
Limit:                        74186752
InUse:                        19669248
MaxInUse:                     19669248
NumAllocs:                          29
MaxAllocSize:                 16777216
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-05-31 22:00:52.574367: W tensorflow/core/common_runtime/bfc_allocator.cc:441] *****_________**********************************************************xxxxxxxxxxxxxxxxxxxxxxxxxxxx
2023-05-31 22:00:52.574408: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at batch_matmul_op_impl.h:685 : Resource exhausted: OOM when allocating tensor with shape[10,512,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
sys.args:  ['exp_train_GCNClassifier.py', '10', 'data_poison', '1024-512-128', '0.0001', 'GCNClassifier', '5%_poison_connlabcfg', '200']
+ loaded padded_train Benign
+ loaded padded_train Malware
+ loaded train dataset
+ loaded padded_test Benign
+ loaded padded_test Malware
+ loaded test dataset
created Graph GCN model
+ model: 
 <util.models.GCN object at 0x7f41b985f970>
Traceback (most recent call last):
  File "exp_train_GCNClassifier.py", line 221, in <module>
    main(sys.argv[1:])
  File "exp_train_GCNClassifier.py", line 205, in main
    train_GCNClassifier()
  File "exp_train_GCNClassifier.py", line 66, in train_GCNClassifier
    output = model.call((batch_feats, batch_adjs), training=True)
  File "/home/ycchiu/git-rep/CFGE2BA/CFGExplainer/util/models.py", line 42, in call
    out = self.getNodeEmb(inputs,training)
  File "/home/ycchiu/git-rep/CFGE2BA/CFGExplainer/util/models.py", line 58, in getNodeEmb
    x = layer.call((x,support),training)
  File "/home/ycchiu/git-rep/CFGE2BA/CFGExplainer/util/layers.py", line 40, in call
    output = tf.matmul(output, self.weight)
  File "/home/ycchiu/anaconda3/envs/ycc3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py", line 201, in wrapper
    return target(*args, **kwargs)
  File "/home/ycchiu/anaconda3/envs/ycc3/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py", line 3276, in matmul
    return gen_math_ops.batch_mat_mul_v2(
  File "/home/ycchiu/anaconda3/envs/ycc3/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py", line 1519, in batch_mat_mul_v2
    _ops.raise_from_not_ok_status(e, name)
  File "/home/ycchiu/anaconda3/envs/ycc3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py", line 6862, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File "<string>", line 3, in raise_from
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[10,512,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:BatchMatMulV2]
